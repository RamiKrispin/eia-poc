---
title: "The US48 Deamnd for Electricity Data Pipeline"
format:
  html:
    code-fold: false
jupyter: python3 
---

This doc run the data refresh process for the US48 demand for electricity dataset from the EIA API. It includes the following functionality:

- Check if new data available on the API
- Setting the data request parameters and pull the data
- Data quality checks 
- Appending the new data and saving the data
- Updating the log file 

## Load libraries

We will pull the data from the EIA API using a set of functions on the `eia_api.py` file. This includes the following functions:

- `eia_get` - A function for query data from the API. Can pull up to 5000 rows per call
- `eia_backfile` - A wrapper function, using batches requests from the API using the `eia_get` function to serve large requests (more than 5000 rows)
- `day_offset` - A helper function creates a vector of dates equally spaced by days
- `hour_offset` - A helper function creates a vector of dates equally spaced by days

The `eia_api.py` file imports the following libraries:

- `pandas` - for data processing
- `datetime` - to work with dates and time objects
- `requests` - to send `GET` requests to the EIA API


In addition, we will use the following libraries:

- `os` - load environment variables
- `numpy` - to create sequences (vectors)
- `plotly` - visualize the data
- `pathlib` - set file path
- `ydata_profiling` - for data profiling


```{python}
#| label: load libraries
import eia_api
import os
import datetime
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from statistics import mean
from pathlib import Path
from ydata_profiling import ProfileReport
from zoneinfo import ZoneInfo
from darts import TimeSeries
from darts.models.forecasting.linear_regression_model import LinearRegressionModel
```


## Load the Log File and API Metadata

We will load the log file from the `data` folder:

```{python}
log_file = pd.read_csv("data/us48_metadata.csv")

cols_time = log_file.columns[3:8]
log_file[cols_time] = log_file[cols_time].apply(pd.to_datetime)
print(log_file.dtypes)

log_file.head

```

Next, we will extract from the log file the infromation of the most recent data refresh:
```{python}
meta_file = log_file[log_file["success"] == True] 
meta_file = meta_file[meta_file["index"] == meta_file["index"].max()]

end_act = meta_file["end_act"]
print(end_act)
```

The `end_act` reprecent the series ending timestamp, we will use it to check if new data is available on the API and set the `start` argument:


```{python}
start = end_act + datetime.timedelta(hours = 1)
start = datetime.datetime.strptime(str(start.iloc[0]), "%Y-%m-%d %H:%M:%S")
start
```


Setting the GET request parameters:

```{python}
api_path = "electricity/rto/region-data/data"
facets = {
    "respondent": "US48",
    "type": "D"
}

offset = 2250

eia_api_key = os.getenv('EIA_API_KEY')
```

Pulling the series metadata from the API. We will use it to check if new data is available on the API using the `endPeriod` variable:

```{python}
metadata = eia_api.eia_metadata(api_key = eia_api_key, api_path = "electricity/rto/region-data/")
```

```{python}
print(metadata.meta["endPeriod"])
end_api = datetime.datetime.strptime(metadata.meta["endPeriod"], "%Y-%m-%dT%H")
type(end_api)
```

The `start` variable represents the data request starting timestamp (calculated as most recent timestamp on the current dataset plus one hour). The `end_api` variable represents the timestamp of the most recent data point available on the API. There is some issue on the API timestampe, a gap of 12 hours between the value of the last data point and the actual, therefore, creating a fix with the `end_fix` variable:


Comparing between the start and 
```{python}
end_fix = end_api -  datetime.timedelta(hours = 12)

print(start)
print(end_fix)
```

We will initiat a GET request of the `end_fix` time is greater than the `start` time: 

```{python}
if start <= end_fix:
    print("Updates available")
    df = eia_api.eia_get(api_key = eia_api_key, 
        api_path = api_path, 
        facets = facets, 
        start = start,
        end = end_fix) 
    update_flag = True
    if(len(df.data) > 0):
        start_match_flag = df.data["period"].min() == start
        end_match_flag = df.data["period"].max() == end_fix
        start_act = df.data["period"].min()
        end_act = df.data["period"].max()
        n_obs = len(df.data)
        na = df.data["value"].isna().sum()
        if start_match_flag and end_match_flag and na == 0 and n_obs > 0:
            print("Refresh successed")
            success_flag = True
        else:
            success_flag = False
            print("Refresh failed")
    else:
        print("Refresh failed")
        success_flag = False
        start_match_flag = None
        end_match_flag = None
        start_act = None
        end_act = None
        n_obs = None
        na = None
        
else:
    print("No updates are available...")
    update_flag = False
    success_flag = False
    start_match_flag = None
    end_match_flag = None
    start_act = None
    end_act = None
    n_obs = None
    na = None

```

## Updating the Dataset and Log File

If the pull was successful, we will append the new data to the current one. In any case, we will update log file with the results of the GET request:
```{python}
if success_flag:
    print(df.data)
```

```{python}
log = {
    "index": log_file["index"].max() + 1,
    "respondent": "US48",
    "respondent_type": "Demand",
    "time": datetime.datetime.now(),
    "start": start,
    "end": end_fix,
    "start_act": start_act,
    "end_act": end_act,
    "start_match": start_match_flag, 
    "end_match": end_match_flag, 
    "n_obs": n_obs,
    "na": na,
    "type": "refresh",
    "update": update_flag,
    "success": success_flag

}

log_file_new = pd.DataFrame([log])

log_file_new
```

```{python}
if (success_flag):
    print("Save the data into CSV file")
    data = pd.read_csv("data/us48.csv")
    data["period"] = pd.to_datetime(data["period"])
    data["value"] = pd.to_numeric(data["value"])
    new_data = data._append(df.data)
    new_data = new_data.sort_values("period")
    new_data.to_csv("data/us48.csv", index = False)
    print("Save the metadata into CSV file")
    new_log = log_file._append(log_file_new)
    new_log.to_csv("data/us48_metadata.csv", index = False)
else:
    if not update_flag:
       print("Updates are not available")
    elif update_flag and not success_flag:
        print("Fail to refresh the data, check the log...")
    new_log = log_file._append(log_file_new)
    new_log.to_csv("data/us48_metadata.csv", index = False)

```

```{python}
new_log
```

## Forecast Refresh

Forecast scope:
- Used a train forecasting model and refit it with new data
- Forecast refresh daily at the end of the day (past 23:00 UTC)
- Forecast horizon 24 hours

Steps:
- Load the forecast log file
- Check the time stamp of the most recent forecast
- Check if new data meet the refresh criteria
- If meet the criteria, update the forecast
- Update the log file



Forecast parameters

```{python}
h = 24
freq = 24
num_sample = 500 
```


Forecast model:
```{python}
lr_model = LinearRegressionModel(lags= [ -freq, -7 * freq,  - 365 * freq],
                                 likelihood= "quantile", 
                                 quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])


```


``` {python}
fc_meta = pd.read_csv("data/fc48_metadata.csv")
fc_meta["start_act"] = pd.to_datetime(fc_meta["start_act"])
fc_meta["end_act"] = pd.to_datetime(fc_meta["end_act"])
fc_meta = fc_meta[fc_meta["score"] == False]
fc_meta.head()
```


Score past models:
```{python}
act_max = new_data["period"].max()

fc_archive = pd.read_csv("data/fc48.csv")
fc_archive["period"] = pd.to_datetime(fc_archive["period"])


for index, row in fc_meta.iterrows():
    if row["start_act"] < act_max:
        label = row["label"]
        n_obs = row["n_obs"]
        f = fc_archive[(fc_archive["label"] == label) & (fc_archive["period"] <= act_max)].merge(new_data[["period", "value"]], on = "period", how = "left")
        fc_meta.at[index, "mape"] = mean(abs(f["value"] - f["mean"]) / f["value"])
        fc_meta.at[index, "rmse"] = (mean((f["value"] - f["mean"]) ** 2 )) ** 0.5
        fc_meta.at[index, "coverage"] =sum((f["value"] <= f["upper"]) & (f["value"] >= f["lower"])) / len(f)

        if len(f) == n_obs: 
            fc_meta.at[index, "score"] = True

```




```{python}
fc_last = fc_meta["end_act"].max()

if fc_last < act_max:
    forecast_start = new_data["period"].max().floor(freq = "d")
    ts_start = forecast_start - datetime.timedelta(hours = freq * 365 * 2)
    d1 = new_data[(new_data["period"] > ts_start) & (new_data["period"] < forecast_start)]
    ts_obj = pd.DataFrame(np.arange(start = d1["period"].min(), stop = d1["period"].max() + datetime.timedelta(hours = 1), step = datetime.timedelta(hours = 1)).astype(datetime.datetime), columns=["index"])
    ts_obj  = ts_obj.merge(d1, left_on = "index", right_on = "period", how="left")
    input = TimeSeries.from_dataframe(d1,time_col= "period", value_cols= "value")
    lr_model.fit(input)
    lr_preds = lr_model.predict(series=input, 
                            n = h, 
                            num_samples = num_sample )

    lr_preds.plot(label = "Forecast",low_quantile=0.05, high_quantile=0.95)
    pred = lr_preds.pd_dataframe()
    fc = pred.quantile(axis = 1, q = [0.05, 0.5, 0.95]).transpose().reset_index()
    fc = fc.rename(columns = {0.05: "lower", 0.5: "mean", 0.95: "upper"})
    label = str(forecast_start.date())

    fc["label"] = label


    fc_log = {
        "time": datetime.datetime.now(tz=ZoneInfo("UTC")).strftime('%Y-%m-%d %H:%M:%S'),
        "label": label,
        "start": forecast_start,
        "start_act": fc["period"].min(),
        "end_act": fc["period"].max(),
        "h": h,
        "n_obs": len(fc),
        "start_flag": forecast_start == fc["period"].min(),
        "n_obs_flag": h == len(fc),
        "model": "LinearRegressionModel",
        "pi":  0.95,
        "score": False,
        "mape": None,
        "rmse": None,
        "coverage": None
    }

    fc_log["success"] = fc_log["start_flag"] and fc_log["n_obs_flag"]
    
    
    print("Save the forecast metadata into CSV file")
    fc_log_df = pd.DataFrame([fc_log])
    fc_log_new = fc_meta._append(fc_log_df)
    fc_log_new.to_csv("data/fc48_metadata.csv", index = False)
   
    print("Save the forecast into CSV file")
    fc_archive = pd.read_csv("data/fc48.csv")
    fc_archive["period"] = pd.to_datetime(fc_archive["period"])
    new_fc_file = fc_archive._append(fc)
    new_fc_file.to_csv("data/fc48.csv", index = False)
     
```