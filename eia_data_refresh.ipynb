{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: The US48 Deamnd for Electricity Data Pipeline\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "---"
      ],
      "id": "3f4b019c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This doc run the data refresh process for the US48 demand for electricity dataset from the EIA API. It includes the following functionality:\n",
        "\n",
        "- Check if new data available on the API\n",
        "- Setting the data request parameters and pull the data\n",
        "- Data quality checks \n",
        "- Appending the new data and saving the data\n",
        "- Updating the log file \n",
        "\n",
        "## Load libraries\n",
        "\n",
        "We will pull the data from the EIA API using a set of functions on the `eia_api.py` file. This includes the following functions:\n",
        "\n",
        "- `eia_get` - A function for query data from the API. Can pull up to 5000 rows per call\n",
        "- `eia_backfile` - A wrapper function, using batches requests from the API using the `eia_get` function to serve large requests (more than 5000 rows)\n",
        "- `day_offset` - A helper function creates a vector of dates equally spaced by days\n",
        "- `hour_offset` - A helper function creates a vector of dates equally spaced by days\n",
        "\n",
        "The `eia_api.py` file imports the following libraries:\n",
        "\n",
        "- `pandas` - for data processing\n",
        "- `datetime` - to work with dates and time objects\n",
        "- `requests` - to send `GET` requests to the EIA API\n",
        "\n",
        "\n",
        "In addition, we will use the following libraries:\n",
        "\n",
        "- `os` - load environment variables\n",
        "- `numpy` - to create sequences (vectors)\n",
        "- `plotly` - visualize the data\n",
        "- `pathlib` - set file path\n",
        "- `ydata_profiling` - for data profiling\n"
      ],
      "id": "60b3f30b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load libraries\n",
        "import eia_api\n",
        "import eia_etl as etl\n",
        "import eia_forecast as fc\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from statistics import mean\n",
        "from pathlib import Path\n",
        "from ydata_profiling import ProfileReport\n",
        "from zoneinfo import ZoneInfo\n",
        "from darts import TimeSeries\n",
        "from darts.models.forecasting.linear_regression_model import LinearRegressionModel"
      ],
      "id": "load-libraries",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Log File and API Metadata\n",
        "\n",
        "We will load the log file from the `data` folder:\n"
      ],
      "id": "7d436aa9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_file = etl.load_log(path = \"data/us48_metadata.csv\")"
      ],
      "id": "5d651796",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `load_log` function returns the following objects:\n",
        "\n",
        "- `log` - the full data log\n",
        "- `last_success` - the log of the last success data pull\n",
        "- `end` - the timestamp of the last data point derived from the `last_success` row \n",
        "- `start` - the start timestamp of the future data pull (calculated as `end` + 1 hour)\n"
      ],
      "id": "16ae92ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(log_file.last_success)\n",
        "print(log_file.end)\n",
        "print(log_file.start)\n",
        "\n",
        "start = log_file.start"
      ],
      "id": "a3dd65d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting the GET request parameters:\n"
      ],
      "id": "3bb86f2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "api_path = \"electricity/rto/region-data/data\"\n",
        "facets = {\n",
        "    \"respondent\": \"US48\",\n",
        "    \"type\": \"D\"\n",
        "}\n",
        "\n",
        "offset = 2250\n",
        "\n",
        "eia_api_key = os.getenv('EIA_API_KEY')"
      ],
      "id": "da4e96ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pulling the series metadata from the API. We will use it to check if new data is available on the API using the `get_api_end` function:\n"
      ],
      "id": "affff0cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metadata = etl.get_api_end(api_key = eia_api_key, \n",
        "api_path = \"electricity/rto/region-data/\", offset = 16)"
      ],
      "id": "8a1d264e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `end` variable represents the timestamp of the most recent data point available on the API. There is some issue on the API timestampe, a gap of 16 hours between the value of the last data point and the actual, therefore, we will offset the `end` value by 16 hours using the `offset` argument and save it as `end_fix`:\n"
      ],
      "id": "a81737f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(metadata.end)\n",
        "print(metadata.end_fix)\n",
        "\n",
        "end_fix = metadata.end_fix"
      ],
      "id": "81fe82cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "new_data = etl.eia_data_refresh(start = start, end = end_fix, api_key = eia_api_key, api_path = api_path, facets = facets) "
      ],
      "id": "ed293bdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updating the Dataset and Log File\n"
      ],
      "id": "90151dc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "append_data = etl.append_new_data(data_path = \"data/us48.csv\",\n",
        "                           log_path = \"data/us48_metadata.csv\",\n",
        "                           new_data = new_data,\n",
        "                           save = True)\n",
        "\n",
        "data = append_data.data"
      ],
      "id": "477a14ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(append_data.log)\n",
        "print(append_data.data_update)"
      ],
      "id": "b51f1476",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecast Refresh\n",
        "\n",
        "Forecast scope:\n",
        "- Used a train forecasting model and refit it with new data\n",
        "- Forecast refresh daily at the end of the day (past 23:00 UTC)\n",
        "- Forecast horizon 24 hours\n",
        "\n",
        "Steps:\n",
        "- Load the forecast log file\n",
        "- Check the time stamp of the most recent forecast\n",
        "- Check if new data meet the refresh criteria\n",
        "- If meet the criteria, update the forecast\n",
        "- Update the log file\n",
        "\n",
        "\n",
        "\n",
        "Forecast parameters\n"
      ],
      "id": "613b0095"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h = 24\n",
        "freq = 24\n",
        "num_sample = 500 "
      ],
      "id": "dcb087e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_path = \"data/fc48_metadata.csv\"\n",
        "fc_path = \"data/fc48.csv\"\n",
        "\n",
        "forecast_start = data[\"period\"].max().floor(freq = \"d\")\n",
        "last_start = fc.get_last_fc_start(log_path = log_path)\n",
        "\n",
        "if last_start < forecast_start:\n",
        "    end = forecast_start - datetime.timedelta(hours = 1)\n",
        "    start = end - datetime.timedelta(hours = freq * 365 * 2)\n",
        "    print(start, end)\n",
        "    input = fc.set_input(input = data, start = start, end = end)\n",
        "    \n",
        "    forecast = fc.train_lm(input = input, \n",
        "            lags = [ -freq, -7 * freq,  - 365 * freq],\n",
        "            likelihood = \"quantile\",\n",
        "            quantiles = [0.025, 0.1, 0.25, 0.5, 0.75, 0.9, 0.975],\n",
        "            h = h,\n",
        "            pi = 0.95,\n",
        "            num_samples = 500)\n",
        "    print(forecast.log)\n",
        "    log = fc.append_log(log_path= log_path, new_log = forecast.log, save = True, init = False)\n",
        "    new_fc = fc.append_forecast(fc_path =  \"data/fc48.csv\", fc_new = forecast, save = True, init = False)"
      ],
      "id": "0de3f788",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Score past forecasts\n"
      ],
      "id": "a6681a6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fc = fc.load_forecast(fc_path)\n",
        "new_log = fc.score_forecast(log_path = \"data/fc48_metadata.csv\", actual = data, forecast = new_fc, save = True)"
      ],
      "id": "6e6e87ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_path = \"data/fc48_metadata.csv\"\n",
        "\n",
        "# def get_last_fc_start(log_path):\n",
        "    \n",
        "fc_log = pd.read_csv(log_path)\n",
        "fc_log = fc_log[fc_log[\"success\"] == True]\n",
        "fc_log = fc_log[fc_log[\"index\"] == fc_log[\"index\"].max()]\n",
        "fc_log[\"start_act\"] = pd.to_datetime(fc_log[\"start_act\"])\n",
        "fc_log[\"end_act\"] = pd.to_datetime(fc_log[\"end_act\"])\n",
        "\n",
        "    # return fc_log\n",
        "    \n",
        "last_start = get_last_fc_start(log_path = log_path)\n",
        "# print(last_start)\n",
        "\n",
        "# forecast_start = data[\"period\"].max().floor(freq = \"d\")\n",
        "\n",
        "# if last_start < pd.to_datetime(forecast_start):\n",
        "#     print(\"test\")"
      ],
      "id": "54742b5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Forecast model:"
      ],
      "id": "0d26df5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lr_model = LinearRegressionModel(lags= [ -freq, -7 * freq,  - 365 * freq],\n",
        "                                 likelihood= \"quantile\", \n",
        "                                 quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\n"
      ],
      "id": "26080740",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fc_meta = pd.read_csv(\"data/fc48_metadata.csv\")\n",
        "fc_meta[\"start_act\"] = pd.to_datetime(fc_meta[\"start_act\"])\n",
        "fc_meta[\"end_act\"] = pd.to_datetime(fc_meta[\"end_act\"])\n",
        "fc_meta = fc_meta[fc_meta[\"score\"] == False]\n",
        "fc_meta.head()"
      ],
      "id": "dce794a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Score past models:"
      ],
      "id": "727c297e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "act_max = data[\"period\"].max()\n",
        "\n",
        "fc_archive = pd.read_csv(\"data/fc48.csv\")\n",
        "fc_archive[\"period\"] = pd.to_datetime(fc_archive[\"period\"])\n",
        "update_fc_meta = False\n",
        "\n",
        "for index, row in fc_meta.iterrows():\n",
        "    if row[\"start_act\"] < act_max:\n",
        "        label = row[\"label\"]\n",
        "        n_obs = row[\"n_obs\"]\n",
        "        f = fc_archive[(fc_archive[\"label\"] == label) & (fc_archive[\"period\"] <= act_max)].merge(data[[\"period\", \"value\"]], on = \"period\", how = \"left\")\n",
        "        fc_meta.at[index, \"mape\"] = mean(abs(f[\"value\"] - f[\"mean\"]) / f[\"value\"])\n",
        "        fc_meta.at[index, \"rmse\"] = (mean((f[\"value\"] - f[\"mean\"]) ** 2 )) ** 0.5\n",
        "        fc_meta.at[index, \"coverage\"] =sum((f[\"value\"] <= f[\"upper\"]) & (f[\"value\"] >= f[\"lower\"])) / len(f)\n",
        "\n",
        "        if len(f) == n_obs: \n",
        "            fc_meta.at[index, \"score\"] = True\n",
        "        update_fc_meta = True\n"
      ],
      "id": "29c1bf42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "forecast_start = data[\"period\"].max().floor(freq = \"d\")"
      ],
      "id": "afc9109d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fc_last = fc_meta[\"end_act\"].max()\n",
        "\n",
        "if fc_last < act_max:\n",
        "    forecast_start = data[\"period\"].max().floor(freq = \"d\")\n",
        "    ts_start = forecast_start - datetime.timedelta(hours = freq * 365 * 2)\n",
        "    d1 = data[(data[\"period\"] > ts_start) & (data[\"period\"] < forecast_start)]\n",
        "    ts_obj = pd.DataFrame(np.arange(start = d1[\"period\"].min(), stop = d1[\"period\"].max() + datetime.timedelta(hours = 1), step = datetime.timedelta(hours = 1)).astype(datetime.datetime), columns=[\"index\"])\n",
        "    ts_obj  = ts_obj.merge(d1, left_on = \"index\", right_on = \"period\", how=\"left\")\n",
        "    input = TimeSeries.from_dataframe(d1,time_col= \"period\", value_cols= \"value\")\n",
        "    lr_model.fit(input)\n",
        "    lr_preds = lr_model.predict(series=input, \n",
        "                            n = h, \n",
        "                            num_samples = num_sample )\n",
        "\n",
        "    lr_preds.plot(label = \"Forecast\",low_quantile=0.05, high_quantile=0.95)\n",
        "    pred = lr_preds.pd_dataframe()\n",
        "    fc = pred.quantile(axis = 1, q = [0.05, 0.5, 0.95]).transpose().reset_index()\n",
        "    fc = fc.rename(columns = {0.05: \"lower\", 0.5: \"mean\", 0.95: \"upper\"})\n",
        "    label = str(forecast_start.date())\n",
        "\n",
        "    fc[\"label\"] = label\n",
        "\n",
        "\n",
        "    fc_log = {\n",
        "        \"time\": datetime.datetime.now(tz=ZoneInfo(\"UTC\")).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        \"label\": label,\n",
        "        \"start\": forecast_start,\n",
        "        \"start_act\": fc[\"period\"].min(),\n",
        "        \"end_act\": fc[\"period\"].max(),\n",
        "        \"h\": h,\n",
        "        \"n_obs\": len(fc),\n",
        "        \"start_flag\": forecast_start == fc[\"period\"].min(),\n",
        "        \"n_obs_flag\": h == len(fc),\n",
        "        \"model\": \"LinearRegressionModel\",\n",
        "        \"pi\":  0.95,\n",
        "        \"score\": False,\n",
        "        \"mape\": None,\n",
        "        \"rmse\": None,\n",
        "        \"coverage\": None\n",
        "    }\n",
        "\n",
        "    fc_log[\"success\"] = fc_log[\"start_flag\"] and fc_log[\"n_obs_flag\"]\n",
        "    \n",
        "    \n",
        "    print(\"Save the forecast metadata into CSV file\")\n",
        "    fc_log_df = pd.DataFrame([fc_log])\n",
        "    fc_log_new = fc_meta._append(fc_log_df)\n",
        "    fc_log_new.to_csv(\"data/fc48_metadata.csv\", index = False)\n",
        "   \n",
        "    print(\"Save the forecast into CSV file\")\n",
        "    fc_archive = pd.read_csv(\"data/fc48.csv\")\n",
        "    fc_archive[\"period\"] = pd.to_datetime(fc_archive[\"period\"])\n",
        "    new_fc_file = fc_archive._append(fc)\n",
        "    new_fc_file.to_csv(\"data/fc48.csv\", index = False)\n",
        "elif update_fc_meta:\n",
        "    fc_meta.to_csv(\"data/fc48_metadata.csv\", index = False)\n"
      ],
      "id": "44b5bbfa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}